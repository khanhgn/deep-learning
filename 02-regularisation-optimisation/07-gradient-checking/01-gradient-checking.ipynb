{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def init_params(layer_dims):\n",
    "    params = {}\n",
    "    np.random.seed(42)\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        params[f\"W{l}\"] = np.random.rand(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        params[f\"b{l}\"] = np.zeros((layer_dims[l],1))\n",
    "    return params\n",
    "\n",
    "def forward_prop(X,params):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(params) // 2 # this gives the number of layers as we have both weights and bias matrices that double the layer count\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        W = params[f\"W{l}\"]\n",
    "        b = params[f\"b{l}\"]\n",
    "        Z = W@A + b\n",
    "        A = sigmoid(Z)\n",
    "        caches.append((A,W,b,Z))\n",
    "    return A, caches\n",
    "\n",
    "\n",
    "def compute_loss(Y,A):\n",
    "    m = Y.shape[1]\n",
    "    loss = -np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))/m\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "\n",
    "def back_prop(X,Y,caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = X.shape[1]\n",
    "    dZ = None\n",
    "\n",
    "    for l in reversed(range(1, L+1)):\n",
    "        A,W,b,Z = caches[l-1]\n",
    "        if l == L:\n",
    "            dZ = A - Y # this is the output layer\n",
    "        else:\n",
    "            dZ = caches[l][1].T@dZ*sigmoid_derivative(Z)\n",
    "        \n",
    "        grads[f\"dW{l}\"] = dZ@caches[l-2][0].T/m if l > 1 else dZ@X.T/m\n",
    "        grads[f\"db{l}\"] = np.sum(dZ,axis=1,keepdims=True) / m\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def optimise(params, grads, learning_rate):\n",
    "    L = len(params) // 2\n",
    "    for l in range(1, L+1):\n",
    "        params[f\"W{l}\"] -= learning_rate*grads[f\"dW{l}\"]\n",
    "        params[f\"b{l}\"] -= learning_rate*grads[f\"db{l}\"]\n",
    "    return params\n",
    "\n",
    "def adam_optimise(params, grads, learning_rate, beta1, beta2, epsilon, t, m, v):\n",
    "    \"\"\"\n",
    "    Updates parameters using the Adam optimization algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        params (dict): Dictionary containing model parameters (e.g., W1, b1, W2, b2, ...).\n",
    "        grads (dict): Dictionary containing gradients of parameters (e.g., dW1, db1, ...).\n",
    "        learning_rate (float): Learning rate for the optimization.\n",
    "        beta1 (float): Exponential decay rate for the first moment estimates.\n",
    "        beta2 (float): Exponential decay rate for the second moment estimates.\n",
    "        epsilon (float): Small value to prevent division by zero.\n",
    "        t (int): Timestep (iteration count).\n",
    "        m (dict): Dictionary to store moving averages of gradients (first moment).\n",
    "        v (dict): Dictionary to store moving averages of squared gradients (second moment).\n",
    "        \n",
    "    Returns:\n",
    "        params (dict): Updated parameters.\n",
    "        m (dict): Updated first moment estimates.\n",
    "        v (dict): Updated second moment estimates.\n",
    "    \"\"\"\n",
    "    L = len(params) // 2  # Number of layers in the neural network\n",
    "\n",
    "    # Update parameters for each layer\n",
    "    for l in range(1, L + 1):\n",
    "        # Compute the moving averages of the gradients (m) and squared gradients (v)\n",
    "        m[f\"dW{l}\"] = beta1 * m.get(f\"dW{l}\", 0) + (1 - beta1) * grads[f\"dW{l}\"]\n",
    "        m[f\"db{l}\"] = beta1 * m.get(f\"db{l}\", 0) + (1 - beta1) * grads[f\"db{l}\"]\n",
    "        v[f\"dW{l}\"] = beta2 * v.get(f\"dW{l}\", 0) + (1 - beta2) * (grads[f\"dW{l}\"] ** 2)\n",
    "        v[f\"db{l}\"] = beta2 * v.get(f\"db{l}\", 0) + (1 - beta2) * (grads[f\"db{l}\"] ** 2)\n",
    "\n",
    "        # Correct bias for first and second moments\n",
    "        m_corrected_dW = m[f\"dW{l}\"] / (1 - beta1 ** t)\n",
    "        m_corrected_db = m[f\"db{l}\"] / (1 - beta1 ** t)\n",
    "        v_corrected_dW = v[f\"dW{l}\"] / (1 - beta2 ** t)\n",
    "        v_corrected_db = v[f\"db{l}\"] / (1 - beta2 ** t)\n",
    "\n",
    "        # Update parameters\n",
    "        params[f\"W{l}\"] -= learning_rate * m_corrected_dW / (v_corrected_dW ** 0.5 + epsilon)\n",
    "        params[f\"b{l}\"] -= learning_rate * m_corrected_db / (v_corrected_db ** 0.5 + epsilon)\n",
    "\n",
    "    return params, m, v\n",
    "\n",
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    layer_dims,\n",
    "    learning_rate=0.1,\n",
    "    num_iterations=1000,\n",
    "    optimizer=\"sgd\",  # Choose between \"sgd\" and \"adam\"\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a neural network using forward and backward propagation with an option\n",
    "    to switch between normal SGD and Adam optimization.\n",
    "\n",
    "    Parameters:\n",
    "        X: Input data.\n",
    "        y: Ground truth labels.\n",
    "        layer_dims: List specifying the dimensions of each layer in the network.\n",
    "        learning_rate: The learning rate for optimization (default: 0.1).\n",
    "        num_iterations: Number of iterations for training (default: 1000).\n",
    "        optimizer: Optimization algorithm to use (\"sgd\" or \"adam\").\n",
    "        beta1: Exponential decay rate for the first moment estimates (Adam only).\n",
    "        beta2: Exponential decay rate for the second moment estimates (Adam only).\n",
    "        epsilon: Small constant to prevent division by zero (Adam only).\n",
    "\n",
    "    Returns:\n",
    "        params: Trained parameters of the network.\n",
    "    \"\"\"\n",
    "    params = init_params(layer_dims)\n",
    "    m, v = {}, {}  # Initialize Adam-specific variables\n",
    "    t = 0  # Adam timestep counter\n",
    "\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        # Forward propagation\n",
    "        A, caches = forward_prop(X, params)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(y, A)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = back_prop(X, y, caches)\n",
    "\n",
    "        # Optimization step\n",
    "        if optimizer == \"adam\":\n",
    "            t += 1\n",
    "            params, m, v = adam_optimise(\n",
    "                params, grads, learning_rate, beta1, beta2, epsilon, t, m, v\n",
    "            )\n",
    "        elif optimizer == \"sgd\":\n",
    "            params = optimise(params, grads, learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer. Choose 'sgd' or 'adam'.\")\n",
    "\n",
    "        # Print loss every 100 iterations\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            print(f\"Iteration [{i}/{num_iterations}], Loss: {loss}\")\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(X, Y, params, grads, layer_dims, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Perform gradient checking to verify the correctness of backprop gradients.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Input data of shape (n_x, m).\n",
    "        Y (ndarray): True \"label\" vector of shape (1, m).\n",
    "        params (dict): Dictionary of your current parameters \"Wl\", \"bl\".\n",
    "        grads (dict): Dictionary of your current gradients \"dWl\", \"dbl\".\n",
    "        layer_dims (list): Dimensions of each layer in the network.\n",
    "        epsilon (float): Small shift to compute numerical gradient.\n",
    "\n",
    "    Returns:\n",
    "        None (Prints out the differences for each parameter)\n",
    "    \"\"\"\n",
    "    # A small helper function to compute cost given parameters\n",
    "    def compute_cost_for_param_set(params):\n",
    "        A, _ = forward_prop(X, params)\n",
    "        cost = compute_loss(Y, A)\n",
    "        return cost\n",
    "    \n",
    "    # Number of layers\n",
    "    L = len(layer_dims) - 1  # If layer_dims = [n_x, n_h, n_y], L=2 (two sets of W, b)\n",
    "\n",
    "    # We will store differences here\n",
    "    differences = []\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        W_key = f\"W{l}\"\n",
    "        b_key = f\"b{l}\"\n",
    "        dW_key = f\"dW{l}\"\n",
    "        db_key = f\"db{l}\"\n",
    "\n",
    "        # ============ Gradient Check for W{l} ============ \n",
    "        w_rows, w_cols = params[W_key].shape\n",
    "        for i in range(w_rows):\n",
    "            for j in range(w_cols):\n",
    "                # Save original value\n",
    "                original_value = params[W_key][i, j]\n",
    "\n",
    "                # J_plus\n",
    "                params[W_key][i, j] = original_value + epsilon\n",
    "                J_plus = compute_cost_for_param_set(params)\n",
    "\n",
    "                # J_minus\n",
    "                params[W_key][i, j] = original_value - epsilon\n",
    "                J_minus = compute_cost_for_param_set(params)\n",
    "\n",
    "                # Reset back to original value\n",
    "                params[W_key][i, j] = original_value\n",
    "\n",
    "                # Numerical approximation of the gradient\n",
    "                grad_approx = (J_plus - J_minus) / (2.0 * epsilon)\n",
    "\n",
    "                # Compare with backprop gradient\n",
    "                grad_backprop = grads[dW_key][i, j]\n",
    "\n",
    "                # Compute relative difference\n",
    "                numerator = abs(grad_approx - grad_backprop)\n",
    "                denominator = abs(grad_approx) + abs(grad_backprop) + 1e-12  # add small term to avoid /0\n",
    "                difference = numerator / denominator\n",
    "                differences.append(difference)\n",
    "\n",
    "        # ============ Gradient Check for b{l} ============ \n",
    "        b_rows, b_cols = params[b_key].shape\n",
    "        for i in range(b_rows):\n",
    "            for j in range(b_cols):\n",
    "                # Save original value\n",
    "                original_value = params[b_key][i, j]\n",
    "\n",
    "                # J_plus\n",
    "                params[b_key][i, j] = original_value + epsilon\n",
    "                J_plus = compute_cost_for_param_set(params)\n",
    "\n",
    "                # J_minus\n",
    "                params[b_key][i, j] = original_value - epsilon\n",
    "                J_minus = compute_cost_for_param_set(params)\n",
    "\n",
    "                # Reset back to original value\n",
    "                params[b_key][i, j] = original_value\n",
    "\n",
    "                # Numerical approximation of the gradient\n",
    "                grad_approx = (J_plus - J_minus) / (2.0 * epsilon)\n",
    "\n",
    "                # Compare with backprop gradient\n",
    "                grad_backprop = grads[db_key][i, j]\n",
    "\n",
    "                # Compute relative difference\n",
    "                numerator = abs(grad_approx - grad_backprop)\n",
    "                denominator = abs(grad_approx) + abs(grad_backprop) + 1e-12\n",
    "                difference = numerator / denominator\n",
    "                differences.append(difference)\n",
    "\n",
    "    # Report results\n",
    "    max_diff = np.max(differences)\n",
    "    print(f\"Max difference between numerical and backprop gradients: {max_diff}\")\n",
    "    if max_diff < 1e-7:\n",
    "        print(\"Your backprop implementation seems to be correct!\")\n",
    "    else:\n",
    "        print(\"There might be an issue with your backprop implementation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference between numerical and backprop gradients: 0.21347565474271563\n",
      "There might be an issue with your backprop implementation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate toy data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(2, 500)  # 2 features, 500 samples\n",
    "y = (X[1, :] > np.sin(2 * np.pi * X[0, :])).astype(int).reshape(1, 500)  # 1 if above the sine wave, else 0\n",
    "layer_dims = [2, 50, 10, 5, 1]\n",
    "# After initializing params:\n",
    "params = init_params(layer_dims)\n",
    "\n",
    "# Forward prop\n",
    "A, caches = forward_prop(X, params)\n",
    "loss = compute_loss(y, A)\n",
    "\n",
    "# Backward prop\n",
    "grads = back_prop(X, y, caches)\n",
    "\n",
    "# Run gradient check\n",
    "gradient_check(X, y, params, grads, layer_dims, epsilon=1e-7)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
