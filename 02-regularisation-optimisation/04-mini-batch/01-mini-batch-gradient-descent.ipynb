{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batches(X, Y, batch_size=64, seed=42):\n",
    "    \"\"\"\n",
    "    Creates a list of random mini-batches from (X, Y).\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, m)\n",
    "    Y -- true labels, of shape (1, m) or (n_y, m)\n",
    "    batch_size -- size of each mini-batch\n",
    "    seed -- random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of tuples (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)  # for reproducibility\n",
    "    m = X.shape[1]        # number of samples\n",
    "    permutation = np.random.permutation(m)\n",
    "\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    mini_batches = []\n",
    "    num_complete_batches = m // batch_size\n",
    "\n",
    "    for i in range(num_complete_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batch_X = shuffled_X[:, start_idx:end_idx]\n",
    "        mini_batch_Y = shuffled_Y[:, start_idx:end_idx]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "\n",
    "    # Handle the last batch if there are leftover samples\n",
    "    if m % batch_size != 0:\n",
    "        start_idx = num_complete_batches * batch_size\n",
    "        mini_batch_X = shuffled_X[:, start_idx:]\n",
    "        mini_batch_Y = shuffled_Y[:, start_idx:]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "\n",
    "    return mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def init_params(layer_dims):\n",
    "    params = {}\n",
    "    np.random.seed(42)\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        params[f\"W{l}\"] = np.random.rand(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        params[f\"b{l}\"] = np.zeros((layer_dims[l],1))\n",
    "    return params\n",
    "\n",
    "def forward_prop(X,params):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(params) // 2 # this gives the number of layers as we have both weights and bias matrices that double the layer count\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        W = params[f\"W{l}\"]\n",
    "        b = params[f\"b{l}\"]\n",
    "        Z = W@A + b\n",
    "        A = sigmoid(Z)\n",
    "        caches.append((A,W,b,Z))\n",
    "    return A, caches\n",
    "\n",
    "\n",
    "def compute_loss(Y,A):\n",
    "    m = Y.shape[1]\n",
    "    loss = -np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))/m\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "\n",
    "def back_prop(X,Y,caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = X.shape[1]\n",
    "    dZ = None\n",
    "\n",
    "    for l in reversed(range(1, L+1)):\n",
    "        A,W,b,Z = caches[l-1]\n",
    "        if l == L:\n",
    "            dZ = A - Y # this is the output layer\n",
    "        else:\n",
    "            dZ = caches[l][1].T@dZ*sigmoid_derivative(Z)\n",
    "        \n",
    "        grads[f\"dW{l}\"] = dZ@caches[l-2][0].T/m if l > 1 else dZ@X.T/m\n",
    "        grads[f\"db{l}\"] = np.sum(dZ,axis=1,keepdims=True) / m\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def optimise(params, grads, learning_rate):\n",
    "    L = len(params) // 2\n",
    "    for l in range(1, L+1):\n",
    "        params[f\"W{l}\"] -= learning_rate*grads[f\"dW{l}\"]\n",
    "        params[f\"b{l}\"] -= learning_rate*grads[f\"db{l}\"]\n",
    "    return params\n",
    "\n",
    "def adam_optimise(params, grads, learning_rate, beta1, beta2, epsilon, t, m, v):\n",
    "    \"\"\"\n",
    "    Updates parameters using the Adam optimization algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        params (dict): Dictionary containing model parameters (e.g., W1, b1, W2, b2, ...).\n",
    "        grads (dict): Dictionary containing gradients of parameters (e.g., dW1, db1, ...).\n",
    "        learning_rate (float): Learning rate for the optimization.\n",
    "        beta1 (float): Exponential decay rate for the first moment estimates.\n",
    "        beta2 (float): Exponential decay rate for the second moment estimates.\n",
    "        epsilon (float): Small value to prevent division by zero.\n",
    "        t (int): Timestep (iteration count).\n",
    "        m (dict): Dictionary to store moving averages of gradients (first moment).\n",
    "        v (dict): Dictionary to store moving averages of squared gradients (second moment).\n",
    "        \n",
    "    Returns:\n",
    "        params (dict): Updated parameters.\n",
    "        m (dict): Updated first moment estimates.\n",
    "        v (dict): Updated second moment estimates.\n",
    "    \"\"\"\n",
    "    L = len(params) // 2  # Number of layers in the neural network\n",
    "\n",
    "    # Update parameters for each layer\n",
    "    for l in range(1, L + 1):\n",
    "        # Compute the moving averages of the gradients (m) and squared gradients (v)\n",
    "        m[f\"dW{l}\"] = beta1 * m.get(f\"dW{l}\", 0) + (1 - beta1) * grads[f\"dW{l}\"]\n",
    "        m[f\"db{l}\"] = beta1 * m.get(f\"db{l}\", 0) + (1 - beta1) * grads[f\"db{l}\"]\n",
    "        v[f\"dW{l}\"] = beta2 * v.get(f\"dW{l}\", 0) + (1 - beta2) * (grads[f\"dW{l}\"] ** 2)\n",
    "        v[f\"db{l}\"] = beta2 * v.get(f\"db{l}\", 0) + (1 - beta2) * (grads[f\"db{l}\"] ** 2)\n",
    "\n",
    "        # Correct bias for first and second moments\n",
    "        m_corrected_dW = m[f\"dW{l}\"] / (1 - beta1 ** t)\n",
    "        m_corrected_db = m[f\"db{l}\"] / (1 - beta1 ** t)\n",
    "        v_corrected_dW = v[f\"dW{l}\"] / (1 - beta2 ** t)\n",
    "        v_corrected_db = v[f\"db{l}\"] / (1 - beta2 ** t)\n",
    "\n",
    "        # Update parameters\n",
    "        params[f\"W{l}\"] -= learning_rate * m_corrected_dW / (v_corrected_dW ** 0.5 + epsilon)\n",
    "        params[f\"b{l}\"] -= learning_rate * m_corrected_db / (v_corrected_db ** 0.5 + epsilon)\n",
    "\n",
    "    return params, m, v\n",
    "\n",
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    layer_dims,\n",
    "    learning_rate=0.1,\n",
    "    num_iterations=1000,\n",
    "    optimizer=\"sgd\",  # Choose between \"sgd\" and \"adam\"\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a neural network using forward and backward propagation with an option\n",
    "    to switch between normal SGD and Adam optimization.\n",
    "\n",
    "    Parameters:\n",
    "        X: Input data.\n",
    "        y: Ground truth labels.\n",
    "        layer_dims: List specifying the dimensions of each layer in the network.\n",
    "        learning_rate: The learning rate for optimization (default: 0.1).\n",
    "        num_iterations: Number of iterations for training (default: 1000).\n",
    "        optimizer: Optimization algorithm to use (\"sgd\" or \"adam\").\n",
    "        beta1: Exponential decay rate for the first moment estimates (Adam only).\n",
    "        beta2: Exponential decay rate for the second moment estimates (Adam only).\n",
    "        epsilon: Small constant to prevent division by zero (Adam only).\n",
    "\n",
    "    Returns:\n",
    "        params: Trained parameters of the network.\n",
    "    \"\"\"\n",
    "    params = init_params(layer_dims)\n",
    "    m, v = {}, {}  # Initialize Adam-specific variables\n",
    "    t = 0  # Adam timestep counter\n",
    "\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        # Forward propagation\n",
    "        A, caches = forward_prop(X, params)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(y, A)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = back_prop(X, y, caches)\n",
    "\n",
    "        # Optimization step\n",
    "        if optimizer == \"adam\":\n",
    "            t += 1\n",
    "            params, m, v = adam_optimise(\n",
    "                params, grads, learning_rate, beta1, beta2, epsilon, t, m, v\n",
    "            )\n",
    "        elif optimizer == \"sgd\":\n",
    "            params = optimise(params, grads, learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer. Choose 'sgd' or 'adam'.\")\n",
    "\n",
    "        # Print loss every 100 iterations\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            print(f\"Iteration [{i}/{num_iterations}], Loss: {loss}\")\n",
    "\n",
    "    return params, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_minibatch(\n",
    "    X, \n",
    "    Y, \n",
    "    layer_dims, \n",
    "    learning_rate=0.01, \n",
    "    num_epochs=1000, \n",
    "    batch_size=64, \n",
    "    optimizer=\"sgd\", \n",
    "    beta1=0.9, \n",
    "    beta2=0.999, \n",
    "    epsilon=1e-8,\n",
    "    print_every=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a neural network using mini-batch gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, m)\n",
    "    Y -- true labels, of shape (1, m) or (n_y, m)\n",
    "    layer_dims -- list describing the layer sizes, e.g. [n_x, n_h, n_y]\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    num_epochs -- number of epochs to run\n",
    "    batch_size -- size of each mini-batch\n",
    "    optimizer -- \"sgd\" or \"adam\"\n",
    "    beta1, beta2, epsilon -- hyperparameters for Adam\n",
    "    print_every -- how often to print the loss\n",
    "\n",
    "    Returns:\n",
    "    params -- the learned parameters after training\n",
    "    \"\"\"\n",
    "    # 1) Initialize parameters\n",
    "    params = init_params(layer_dims)\n",
    "\n",
    "    # For Adam, we also need to keep track of the moving averages m, v\n",
    "    m_dict, v_dict = {}, {}\n",
    "    t = 0  # Adam timestep\n",
    "\n",
    "    # 2) Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Create mini-batches\n",
    "        mini_batches = get_mini_batches(X, Y, batch_size)\n",
    "\n",
    "        for batch_idx, (mini_batch_X, mini_batch_Y) in enumerate(mini_batches, start=1):\n",
    "            # Forward pass\n",
    "            A, caches = forward_prop(mini_batch_X, params)\n",
    "\n",
    "            # Compute loss for this batch\n",
    "            loss = compute_loss(mini_batch_Y, A)\n",
    "\n",
    "            # Backward pass\n",
    "            grads = back_prop(mini_batch_X, mini_batch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"adam\":\n",
    "                t += 1\n",
    "                params, m_dict, v_dict = adam_optimise(\n",
    "                    params, grads, learning_rate, beta1, beta2, epsilon, t, m_dict, v_dict\n",
    "                )\n",
    "            elif optimizer == \"sgd\":\n",
    "                params = optimise(params, grads, learning_rate)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported optimizer. Choose 'sgd' or 'adam'.\")\n",
    "\n",
    "        # (Optional) Print loss every 'print_every' epochs\n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "            # You can evaluate the loss on the full dataset here (or just the last batch)\n",
    "            A_full, _ = forward_prop(X, params)\n",
    "            loss_full = compute_loss(Y, A_full)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss on full data: {loss_full:.4f}\")\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate toy data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(2, 2000)  # 2 features, 500 samples\n",
    "Y = (X[1, :] > np.sin(2 * np.pi * X[0, :])).astype(int).reshape(1, 2000)  # 1 if above the sine wave, else 0\n",
    "\n",
    "# Define layer dimensions\n",
    "layer_dims = [2, 100, 100, 1]  # Simplified for visualization\n",
    "\n",
    "# Train the model\n",
    "parameters = train_with_minibatch(X, Y, layer_dims, learning_rate=0.1, num_epochs=1000, batch_size=100)\n",
    "\n",
    "# Generate a grid of points for visualization\n",
    "x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# Predict on the grid\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()].T\n",
    "Z, _ = forward_prop(grid_points, parameters)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.8, cmap=plt.cm.Spectral)\n",
    "plt.scatter(X[0, :], X[1, :], c=Y.ravel(), edgecolors='k', cmap=plt.cm.Spectral)\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
