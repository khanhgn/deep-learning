{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def init_params(layer_dims):\n",
    "    params = {}\n",
    "    np.random.seed(42)\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        params[f\"W{l}\"] = np.random.rand(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        params[f\"b{l}\"] = np.zeros((layer_dims[l],1))\n",
    "    return params\n",
    "\n",
    "def forward_prop(X,params):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(params) // 2 # this gives the number of layers as we have both weights and bias matrices that double the layer count\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        W = params[f\"W{l}\"]\n",
    "        b = params[f\"b{l}\"]\n",
    "        Z = W@A + b\n",
    "        A = sigmoid(Z)\n",
    "        caches.append((A,W,b,Z))\n",
    "    return A, caches\n",
    "\n",
    "\n",
    "def compute_loss(Y,A):\n",
    "    m = Y.shape[1]\n",
    "    loss = -np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))/m\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "\n",
    "def back_prop(X,Y,caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = X.shape[1]\n",
    "    dZ = None\n",
    "\n",
    "    for l in reversed(range(1, L+1)):\n",
    "        A,W,b,Z = caches[l-1]\n",
    "        if l == L:\n",
    "            dZ = A - Y # this is the output layer\n",
    "        else:\n",
    "            dZ = caches[l][1].T@dZ*sigmoid_derivative(Z)\n",
    "        \n",
    "        grads[f\"dW{l}\"] = dZ@caches[l-2][0].T/m if l > 1 else dZ@X.T/m\n",
    "        grads[f\"db{l}\"] = np.sum(dZ,axis=1,keepdims=True) / m\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def optimise(params, grads, learning_rate):\n",
    "    L = len(params) // 2\n",
    "    for l in range(1, L+1):\n",
    "        params[f\"W{l}\"] -= learning_rate*grads[f\"dW{l}\"]\n",
    "        params[f\"b{l}\"] -= learning_rate*grads[f\"db{l}\"]\n",
    "    return params\n",
    "\n",
    "def adam_optimise(params, grads, learning_rate, beta1, beta2, epsilon, t, m, v):\n",
    "    \"\"\"\n",
    "    Updates parameters using the Adam optimization algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        params (dict): Dictionary containing model parameters (e.g., W1, b1, W2, b2, ...).\n",
    "        grads (dict): Dictionary containing gradients of parameters (e.g., dW1, db1, ...).\n",
    "        learning_rate (float): Learning rate for the optimization.\n",
    "        beta1 (float): Exponential decay rate for the first moment estimates.\n",
    "        beta2 (float): Exponential decay rate for the second moment estimates.\n",
    "        epsilon (float): Small value to prevent division by zero.\n",
    "        t (int): Timestep (iteration count).\n",
    "        m (dict): Dictionary to store moving averages of gradients (first moment).\n",
    "        v (dict): Dictionary to store moving averages of squared gradients (second moment).\n",
    "        \n",
    "    Returns:\n",
    "        params (dict): Updated parameters.\n",
    "        m (dict): Updated first moment estimates.\n",
    "        v (dict): Updated second moment estimates.\n",
    "    \"\"\"\n",
    "    L = len(params) // 2  # Number of layers in the neural network\n",
    "\n",
    "    # Update parameters for each layer\n",
    "    for l in range(1, L + 1):\n",
    "        # Compute the moving averages of the gradients (m) and squared gradients (v)\n",
    "        m[f\"dW{l}\"] = beta1 * m.get(f\"dW{l}\", 0) + (1 - beta1) * grads[f\"dW{l}\"]\n",
    "        m[f\"db{l}\"] = beta1 * m.get(f\"db{l}\", 0) + (1 - beta1) * grads[f\"db{l}\"]\n",
    "        v[f\"dW{l}\"] = beta2 * v.get(f\"dW{l}\", 0) + (1 - beta2) * (grads[f\"dW{l}\"] ** 2)\n",
    "        v[f\"db{l}\"] = beta2 * v.get(f\"db{l}\", 0) + (1 - beta2) * (grads[f\"db{l}\"] ** 2)\n",
    "\n",
    "        # Correct bias for first and second moments\n",
    "        m_corrected_dW = m[f\"dW{l}\"] / (1 - beta1 ** t)\n",
    "        m_corrected_db = m[f\"db{l}\"] / (1 - beta1 ** t)\n",
    "        v_corrected_dW = v[f\"dW{l}\"] / (1 - beta2 ** t)\n",
    "        v_corrected_db = v[f\"db{l}\"] / (1 - beta2 ** t)\n",
    "\n",
    "        # Update parameters\n",
    "        params[f\"W{l}\"] -= learning_rate * m_corrected_dW / (v_corrected_dW ** 0.5 + epsilon)\n",
    "        params[f\"b{l}\"] -= learning_rate * m_corrected_db / (v_corrected_db ** 0.5 + epsilon)\n",
    "\n",
    "    return params, m, v\n",
    "\n",
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    layer_dims,\n",
    "    learning_rate=0.1,\n",
    "    num_iterations=1000,\n",
    "    optimizer=\"sgd\",  # Choose between \"sgd\" and \"adam\"\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a neural network using forward and backward propagation with an option\n",
    "    to switch between normal SGD and Adam optimization.\n",
    "\n",
    "    Parameters:\n",
    "        X: Input data.\n",
    "        y: Ground truth labels.\n",
    "        layer_dims: List specifying the dimensions of each layer in the network.\n",
    "        learning_rate: The learning rate for optimization (default: 0.1).\n",
    "        num_iterations: Number of iterations for training (default: 1000).\n",
    "        optimizer: Optimization algorithm to use (\"sgd\" or \"adam\").\n",
    "        beta1: Exponential decay rate for the first moment estimates (Adam only).\n",
    "        beta2: Exponential decay rate for the second moment estimates (Adam only).\n",
    "        epsilon: Small constant to prevent division by zero (Adam only).\n",
    "\n",
    "    Returns:\n",
    "        params: Trained parameters of the network.\n",
    "    \"\"\"\n",
    "    params = init_params(layer_dims)\n",
    "    m, v = {}, {}  # Initialize Adam-specific variables\n",
    "    t = 0  # Adam timestep counter\n",
    "\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        # Forward propagation\n",
    "        A, caches = forward_prop(X, params)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(y, A)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = back_prop(X, y, caches)\n",
    "\n",
    "        # Optimization step\n",
    "        if optimizer == \"adam\":\n",
    "            t += 1\n",
    "            params, m, v = adam_optimise(\n",
    "                params, grads, learning_rate, beta1, beta2, epsilon, t, m, v\n",
    "            )\n",
    "        elif optimizer == \"sgd\":\n",
    "            params = optimise(params, grads, learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer. Choose 'sgd' or 'adam'.\")\n",
    "\n",
    "        # Print loss every 100 iterations\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            print(f\"Iteration [{i}/{num_iterations}], Loss: {loss}\")\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate toy data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(2, 500)  # 2 features, 500 samples\n",
    "Y = (X[1, :] > np.sin(2 * np.pi * X[0, :])).astype(int).reshape(1, 500)  # 1 if above the sine wave, else 0\n",
    "\n",
    "# Define layer dimensions\n",
    "layer_dims = [2, 100, 100, 1]  # Simplified for visualization\n",
    "\n",
    "# Train the model\n",
    "parameters = train(X, Y, layer_dims, optimizer=\"adam\", learning_rate=0.1, num_iterations=2000)\n",
    "\n",
    "# Generate a grid of points for visualization\n",
    "x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# Predict on the grid\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()].T\n",
    "Z, _ = forward_prop(grid_points, parameters)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.8, cmap=plt.cm.Spectral)\n",
    "plt.scatter(X[0, :], X[1, :], c=Y.ravel(), edgecolors='k', cmap=plt.cm.Spectral)\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop out regularised neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def init_params(layer_dims):\n",
    "    params = {}\n",
    "    np.random.seed(42)\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        params[f\"W{l}\"] = np.random.rand(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        params[f\"b{l}\"] = np.zeros((layer_dims[l],1))\n",
    "    return params\n",
    "\n",
    "def forward_prop(X, params, keep_prop=0.8):\n",
    "    \"\"\"\n",
    "    Forward pass with inverted dropout applied to hidden layers only.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, shape (input_size, number_of_samples)\n",
    "    params -- python dictionary containing your parameters \"Wl\" and \"bl\"\n",
    "    keep_prop -- probability of keeping a neuron active\n",
    "    \n",
    "    Returns:\n",
    "    A -- the output of the last layer (without dropout)\n",
    "    caches -- list of tuples (A, W, b, Z, d) for each layer\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(params) // 2  # total number of layers\n",
    "\n",
    "    # Forward through the first L-1 layers (hidden layers with dropout)\n",
    "    for l in range(1, L):\n",
    "        W = params[f\"W{l}\"]\n",
    "        b = params[f\"b{l}\"]\n",
    "        \n",
    "        # Compute linear + activation\n",
    "        Z = W @ A + b\n",
    "        A = sigmoid(Z)\n",
    "\n",
    "        # Apply dropout mask to the hidden layer\n",
    "        d = (np.random.rand(*A.shape) < keep_prop).astype(float)  # shape matches A\n",
    "        A = (A * d) / keep_prop  # \"inverted dropout\" scaling\n",
    "\n",
    "        caches.append((A, W, b, Z, d))\n",
    "\n",
    "    # Final layer (no dropout)\n",
    "    W = params[f\"W{L}\"]\n",
    "    b = params[f\"b{L}\"]\n",
    "    Z = W @ A + b\n",
    "    A = sigmoid(Z)\n",
    "\n",
    "    # For the final layer, store `d` as None to indicate no dropout\n",
    "    caches.append((A, W, b, Z, None))\n",
    "\n",
    "    return A, caches\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(Y,A):\n",
    "    m = Y.shape[1]\n",
    "    loss = -np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))/m\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "def back_prop(X, Y, caches, keep_prop=0.8):\n",
    "    \"\"\"\n",
    "    Backward pass with dropout applied consistently for hidden layers.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, shape (n_x, m)\n",
    "    Y -- true labels, shape (1, m)\n",
    "    caches -- list of (A, W, b, Z, d) from forward_prop\n",
    "    keep_prop -- probability of keeping a neuron active in dropout\n",
    "\n",
    "    Returns:\n",
    "    grads -- dictionary with gradients {dW1, db1, dW2, db2, ...}\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)  # number of layers (including final)\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # --------------------------\n",
    "    # 1) Backprop through output layer\n",
    "    # caches[-1] = (A_last, W_last, b_last, Z_last, d_last=None)\n",
    "    A_last, W_last, b_last, Z_last, d_last = caches[-1]\n",
    "    dZ_last = A_last - Y  # shape (1, m)\n",
    "\n",
    "    # Gradient for final layer\n",
    "    if L > 1:\n",
    "        A_prev = caches[-2][0]  # A of the previous layer\n",
    "    else:\n",
    "        A_prev = X\n",
    "    grads[f\"dW{L}\"] = (dZ_last @ A_prev.T) / m  # shape => (1, 100)\n",
    "    grads[f\"db{L}\"] = np.sum(dZ_last, axis=1, keepdims=True) / m\n",
    "\n",
    "    # Now pass gradient back as dA for layer L-1\n",
    "    dA_prev = W_last.T @ dZ_last  # shape => (100, m)\n",
    "\n",
    "    # --------------------------\n",
    "    # 2) Backprop through hidden layers in reverse\n",
    "    for l in reversed(range(1, L)):\n",
    "        # caches[l-1] = (A_l, W_l, b_l, Z_l, d_l)\n",
    "        A_l, W_l, b_l, Z_l, d_l = caches[l-1]\n",
    "\n",
    "        # Compute dZ_l = dA_l * derivative(sigmoid(Z_l))\n",
    "        dZ_l = dA_prev * sigmoid_derivative(Z_l)\n",
    "\n",
    "        # Re-apply dropout if not the output layer\n",
    "        if d_l is not None:\n",
    "            dZ_l = (dZ_l * d_l) / keep_prop\n",
    "\n",
    "        # A_{l-1} or X if l=1\n",
    "        if l > 1:\n",
    "            A_prev = caches[l-2][0]  # the output of layer l-1\n",
    "        else:\n",
    "            A_prev = X\n",
    "\n",
    "        # Save grads\n",
    "        grads[f\"dW{l}\"] = (dZ_l @ A_prev.T) / m\n",
    "        grads[f\"db{l}\"] = np.sum(dZ_l, axis=1, keepdims=True) / m\n",
    "\n",
    "        # Pass gradient to the next layer down\n",
    "        # dA_{l-1} = W_l^T @ dZ_l\n",
    "        dA_prev = W_l.T @ dZ_l\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def optimise(params, grads, learning_rate):\n",
    "    L = len(params) // 2\n",
    "    for l in range(1, L+1):\n",
    "        params[f\"W{l}\"] -= learning_rate*grads[f\"dW{l}\"]\n",
    "        params[f\"b{l}\"] -= learning_rate*grads[f\"db{l}\"]\n",
    "    return params\n",
    "\n",
    "def adam_optimise(params, grads, learning_rate, beta1, beta2, epsilon, t, m, v):\n",
    "    \"\"\"\n",
    "    Updates parameters using the Adam optimization algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        params (dict): Dictionary containing model parameters (e.g., W1, b1, W2, b2, ...).\n",
    "        grads (dict): Dictionary containing gradients of parameters (e.g., dW1, db1, ...).\n",
    "        learning_rate (float): Learning rate for the optimization.\n",
    "        beta1 (float): Exponential decay rate for the first moment estimates.\n",
    "        beta2 (float): Exponential decay rate for the second moment estimates.\n",
    "        epsilon (float): Small value to prevent division by zero.\n",
    "        t (int): Timestep (iteration count).\n",
    "        m (dict): Dictionary to store moving averages of gradients (first moment).\n",
    "        v (dict): Dictionary to store moving averages of squared gradients (second moment).\n",
    "        \n",
    "    Returns:\n",
    "        params (dict): Updated parameters.\n",
    "        m (dict): Updated first moment estimates.\n",
    "        v (dict): Updated second moment estimates.\n",
    "    \"\"\"\n",
    "    L = len(params) // 2  # Number of layers in the neural network\n",
    "\n",
    "    # Update parameters for each layer\n",
    "    for l in range(1, L + 1):\n",
    "        # Compute the moving averages of the gradients (m) and squared gradients (v)\n",
    "        m[f\"dW{l}\"] = beta1 * m.get(f\"dW{l}\", 0) + (1 - beta1) * grads[f\"dW{l}\"]\n",
    "        m[f\"db{l}\"] = beta1 * m.get(f\"db{l}\", 0) + (1 - beta1) * grads[f\"db{l}\"]\n",
    "        v[f\"dW{l}\"] = beta2 * v.get(f\"dW{l}\", 0) + (1 - beta2) * (grads[f\"dW{l}\"] ** 2)\n",
    "        v[f\"db{l}\"] = beta2 * v.get(f\"db{l}\", 0) + (1 - beta2) * (grads[f\"db{l}\"] ** 2)\n",
    "\n",
    "        # Correct bias for first and second moments\n",
    "        m_corrected_dW = m[f\"dW{l}\"] / (1 - beta1 ** t)\n",
    "        m_corrected_db = m[f\"db{l}\"] / (1 - beta1 ** t)\n",
    "        v_corrected_dW = v[f\"dW{l}\"] / (1 - beta2 ** t)\n",
    "        v_corrected_db = v[f\"db{l}\"] / (1 - beta2 ** t)\n",
    "\n",
    "        # Update parameters\n",
    "        params[f\"W{l}\"] -= learning_rate * m_corrected_dW / (v_corrected_dW ** 0.5 + epsilon)\n",
    "        params[f\"b{l}\"] -= learning_rate * m_corrected_db / (v_corrected_db ** 0.5 + epsilon)\n",
    "\n",
    "    return params, m, v\n",
    "\n",
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    layer_dims,\n",
    "    learning_rate=0.1,\n",
    "    num_iterations=1000,\n",
    "    optimizer=\"sgd\",  # Choose between \"sgd\" and \"adam\"\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-8,\n",
    "    keep_prop=0.8\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a neural network using forward and backward propagation with an option\n",
    "    to switch between normal SGD and Adam optimization.\n",
    "\n",
    "    Parameters:\n",
    "        X: Input data.\n",
    "        y: Ground truth labels.\n",
    "        layer_dims: List specifying the dimensions of each layer in the network.\n",
    "        learning_rate: The learning rate for optimization (default: 0.1).\n",
    "        num_iterations: Number of iterations for training (default: 1000).\n",
    "        optimizer: Optimization algorithm to use (\"sgd\" or \"adam\").\n",
    "        beta1: Exponential decay rate for the first moment estimates (Adam only).\n",
    "        beta2: Exponential decay rate for the second moment estimates (Adam only).\n",
    "        epsilon: Small constant to prevent division by zero (Adam only).\n",
    "\n",
    "    Returns:\n",
    "        params: Trained parameters of the network.\n",
    "    \"\"\"\n",
    "    params = init_params(layer_dims)\n",
    "    m, v = {}, {}  # Initialize Adam-specific variables\n",
    "    t = 0  # Adam timestep counter\n",
    "\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        # Forward propagation\n",
    "        A, caches = forward_prop(X, params, keep_prop)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(y, A)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = back_prop(X, y, caches, keep_prop)\n",
    "\n",
    "        # Optimization step\n",
    "        if optimizer == \"adam\":\n",
    "            t += 1\n",
    "            params, m, v = adam_optimise(\n",
    "                params, grads, learning_rate, beta1, beta2, epsilon, t, m, v\n",
    "            )\n",
    "        elif optimizer == \"sgd\":\n",
    "            params = optimise(params, grads, learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer. Choose 'sgd' or 'adam'.\")\n",
    "\n",
    "        # Print loss every 100 iterations\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            print(f\"Iteration [{i}/{num_iterations}], Loss: {loss}\")\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate toy data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(2, 500)  # 2 features, 500 samples\n",
    "Y = (X[1, :] > np.sin(2 * np.pi * X[0, :])).astype(int).reshape(1, 500)  # 1 if above the sine wave, else 0\n",
    "\n",
    "# Define layer dimensions\n",
    "layer_dims = [2, 100, 100, 1]  # Simplified for visualization\n",
    "\n",
    "# Train the model\n",
    "parameters = train(X, Y, layer_dims, optimizer=\"adam\", learning_rate=0.1, num_iterations=2000, keep_prop=0.8)\n",
    "\n",
    "# Generate a grid of points for visualization\n",
    "x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                     np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "# Predict on the grid\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()].T\n",
    "Z, _ = forward_prop(grid_points, parameters)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.8, cmap=plt.cm.Spectral)\n",
    "plt.scatter(X[0, :], X[1, :], c=Y.ravel(), edgecolors='k', cmap=plt.cm.Spectral)\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
